{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization_DL2019.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leox1v/Optimization_tutorial/blob/master/Optimization_DL2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eZhB2soswkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQKXFIibtYDb",
        "colab_type": "text"
      },
      "source": [
        "# Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiymbqcRtb1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Let's press the data between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZww1SFSvgVo",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLoscLC0vfTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1u5iCbhzceY",
        "colab_type": "text"
      },
      "source": [
        "# SGD\n",
        "$$ \n",
        "    \\theta^+ = \\theta - \\eta \\nabla L(x_{i:i+n}; \\theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwyn9JQqzTYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MySGD(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        super(MySGD, self).__init__(name='my_sgd')\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def apply_gradients(self, grads_and_vars): \n",
        "      # TODO: implement the update step \n",
        "      # hint: use variable.assign_sub(value) to modify the variable value \n",
        "    \n",
        "\n",
        "    def get_config(self):\n",
        "      config = {\n",
        "          'lr': self.learning_rate\n",
        "      }\n",
        "      base_config = super(SGD, self).get_config()\n",
        "      return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6THoN90tzceb",
        "colab_type": "text"
      },
      "source": [
        "# Adagrad \n",
        "$$\n",
        "    g_{t,i} = \\nabla_\\theta L(x_t; \\theta_{t,i})\\\\\n",
        "    G_{t,i} = \\sum_{\\tau=0}^t g_{\\tau, i}^2\\\\\n",
        "    \\theta^+ = \\theta - \\frac{\\eta}{\\sqrt{G_{t,i} + \\epsilon}} g_{t,i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeP5NH8ezceb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyAdagrad(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, epsilon=0.1):\n",
        "        super(MyAdagrad, self).__init__(name='MyAdagrad')\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.g = None\n",
        "        \n",
        "    def apply_gradients(self, grads_and_vars):\n",
        "        # TODO: implement the update step \n",
        "        # hint: use variable.assign_sub(value) to modify the variable value \n",
        "\n",
        "    def get_config(self):\n",
        "      config = {\n",
        "          'lr': self.learning_rate,\n",
        "          'epsilon': self.epsilon\n",
        "      }\n",
        "      base_config = super(SGD, self).get_config()\n",
        "      return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKTvswPYzced",
        "colab_type": "text"
      },
      "source": [
        "# Adam\n",
        "https://arxiv.org/pdf/1412.6980.pdf see improved algorithm at the end of section 2.\n",
        "$$\n",
        "    m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta L(x_t; \\theta_t)\\\\\n",
        "    v_t = \\beta_2 v_{t-1} + (1-\\beta_2) \\left(\\nabla_\\theta L(x_t; \\theta_t)\\right) ^2\\\\\n",
        "    \\eta_t = \\eta * \\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t}\n",
        "$$\n",
        "$$\n",
        "    \\theta_{t+1} = \\theta_t - \\frac{\\eta_t}{\\sqrt{v_t} + \\epsilon}m_t\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfBZCj5tzcee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyAdam(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-07):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.iteration = 0\n",
        "        \n",
        "    def apply_gradients(self, grads_and_vars):\n",
        "        # TODO: implement the update step \n",
        "        # hint: use variable.assign_sub(value) to modify the variable value \n",
        "\n",
        "    def get_config(self):\n",
        "      config = {\n",
        "          'lr': self.learning_rate,\n",
        "          'epsilon': self.epsilon,\n",
        "          'beta1': self.beta1,\n",
        "          'beta2': self.beta2\n",
        "      }\n",
        "      base_config = super(SGD, self).get_config()\n",
        "      return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKqFSoYgvjkh",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYM6aP4Kvxi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SGD\n",
        "model = get_model()\n",
        "model.compile(optimizer=MySGD(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn0k7ZmO--ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adagrad\n",
        "model = get_model()\n",
        "model.compile(optimizer=MyAdagrad(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj46YNz0hmka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adam\n",
        "model = get_model()\n",
        "model.compile(optimizer=MyAdam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}