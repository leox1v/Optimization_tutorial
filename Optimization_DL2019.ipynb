{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization_DL2019.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leox1v/Optimization_tutorial/blob/master/Optimization_DL2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eZhB2soswkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQKXFIibtYDb",
        "colab_type": "text"
      },
      "source": [
        "# Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiymbqcRtb1x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6528b2e4-bff6-4575-e422-3d7a0d5d7647"
      },
      "source": [
        "# get the dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Let's press the data between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZww1SFSvgVo",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLoscLC0vfTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1u5iCbhzceY",
        "colab_type": "text"
      },
      "source": [
        "# SGD\n",
        "$$ \n",
        "    \\theta^+ = \\theta - \\eta \\nabla L(x_{i:i+n}; \\theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwyn9JQqzTYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MySGD(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        super(MySGD, self).__init__(name='my_sgd')\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def apply_gradients(self, grads_and_vars): \n",
        "      # use variable.assign_sub(value) to modify the variable value \n",
        "        for gradient, variable in grads_and_vars:\n",
        "          variable.assign_sub(self.learning_rate*gradient)\n",
        "\n",
        "    def get_config(self):\n",
        "      config = {\n",
        "          'lr': self.learning_rate\n",
        "      }\n",
        "      base_config = super(SGD, self).get_config()\n",
        "      return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6THoN90tzceb",
        "colab_type": "text"
      },
      "source": [
        "# Adagrad \n",
        "$$\n",
        "    g_{t,i} = \\nabla_\\theta L(x_t; \\theta_{t,i})\\\\\n",
        "    G_{t,i} = \\sum_{\\tau=0}^t g_{\\tau, i}^2\\\\\n",
        "    \\theta^+ = \\theta - \\frac{\\eta}{\\sqrt{G_{t,i} + \\epsilon}} g_{t,i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeP5NH8ezceb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyAdagrad(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, epsilon=0.1):\n",
        "        super(MyAdagrad, self).__init__(name='MyAdagrad')\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.g = None\n",
        "        \n",
        "    def apply_gradients(self, grads_and_vars):\n",
        "        if self.g is None:\n",
        "            grads_and_vars = list(grads_and_vars)\n",
        "            [print('Weights {}: gradient of shape {}'.format(i, grad.shape)) for i, (grad, variable) in enumerate(grads_and_vars)]\n",
        "            self.g = [tf.ones_like(gradient_layer, dtype=tf.float32) * self.epsilon for (gradient_layer, _) in grads_and_vars]\n",
        "            \n",
        "        for i, (gradient, variable) in enumerate(grads_and_vars):\n",
        "            self.g[i] += tf.square(gradient)\n",
        "            variable.assign_sub(self.learning_rate/tf.sqrt(self.g[i])*gradient)\n",
        "\n",
        "    def get_config(self):\n",
        "      config = {\n",
        "          'lr': self.learning_rate,\n",
        "          'epsilon': self.epsilon\n",
        "      }\n",
        "      base_config = super(SGD, self).get_config()\n",
        "      return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKTvswPYzced",
        "colab_type": "text"
      },
      "source": [
        "# Adam\n",
        "https://arxiv.org/pdf/1412.6980.pdf see improved algorithm at the end of section 2.\n",
        "$$\n",
        "    m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta L(x_t; \\theta_t)\\\\\n",
        "    v_t = \\beta_2 v_{t-1} + (1-\\beta_2) \\left(\\nabla_\\theta L(x_t; \\theta_t)\\right) ^2\\\\\n",
        "    \\eta_t = \\eta * \\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t}\n",
        "$$\n",
        "$$\n",
        "    \\theta_{t+1} = \\theta_t - \\frac{\\eta_t}{\\sqrt{v_t} + \\epsilon}m_t\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfBZCj5tzcee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyAdam(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-07):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.iteration = 0\n",
        "        \n",
        "    def apply_gradients(self, grads_and_vars):\n",
        "        self.iteration += 1\n",
        "        if self.m is None:\n",
        "            grads_and_vars = list(grads_and_vars)\n",
        "            self.m = [tf.zeros_like(gradient_layer) for (gradient_layer, _) in grads_and_vars]\n",
        "            self.v = [tf.zeros_like(gradient_layer) for (gradient_layer, _) in grads_and_vars]\n",
        "\n",
        "        learning_rate = self.learning_rate * tf.sqrt(1-self.beta2**self.iteration)/(1-self.beta1**self.iteration)\n",
        "        \n",
        "        for i, (gradient, variable) in enumerate(grads_and_vars):\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradient\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * tf.square(gradient)\n",
        "            update_step = learning_rate / (tf.sqrt(self.v[i]) + self.epsilon) * self.m[i]\n",
        "            variable.assign_sub(update_step)\n",
        "\n",
        "    def get_config(self):\n",
        "      config = {\n",
        "          'lr': self.learning_rate,\n",
        "          'epsilon': self.epsilon,\n",
        "          'beta1': self.beta1,\n",
        "          'beta2': self.beta2\n",
        "      }\n",
        "      base_config = super(SGD, self).get_config()\n",
        "      return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKqFSoYgvjkh",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYM6aP4Kvxi9",
        "colab_type": "code",
        "outputId": "30560dad-0042-4eb1-a27c-21ea2739848b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# SGD\n",
        "model = get_model()\n",
        "model.compile(optimizer=MySGD(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/3\n",
            "60000/60000 [==============================] - 6s 92us/sample - loss: 0.6398 - accuracy: 0.8379\n",
            "Epoch 2/3\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.3383 - accuracy: 0.9060\n",
            "Epoch 3/3\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.2906 - accuracy: 0.9175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn0k7ZmO--ia",
        "colab_type": "code",
        "outputId": "40bcbb70-c7d5-4a00-c927-b5379c4d6932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Adagrad\n",
        "model = get_model()\n",
        "model.compile(optimizer=MyAdagrad(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/3\n",
            "Weights 0: gradient of shape (784, 128)\n",
            "Weights 1: gradient of shape (128,)\n",
            "Weights 2: gradient of shape (128, 10)\n",
            "Weights 3: gradient of shape (10,)\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4357 - accuracy: 0.8818\n",
            "Epoch 2/3\n",
            "60000/60000 [==============================] - 6s 92us/sample - loss: 0.2437 - accuracy: 0.9313\n",
            "Epoch 3/3\n",
            "60000/60000 [==============================] - 5s 91us/sample - loss: 0.1917 - accuracy: 0.9462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj46YNz0hmka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "690ce1d0-5cd3-4181-c214-9afa6ba15484"
      },
      "source": [
        "# Adam\n",
        "model = get_model()\n",
        "model.compile(optimizer=MyAdam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=3)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/3\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.3775 - accuracy: 0.9061\n",
            "Epoch 2/3\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.3134 - accuracy: 0.9428\n",
            "Epoch 3/3\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.2757 - accuracy: 0.9534\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}