{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# load mnist images and store them in two seperate tensorflow dataset (train and test)\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train, y_test = tf.cast(y_train, dtype=tf.int32), tf.cast(y_test, dtype=tf.int32)\n",
    "dset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(100, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "dset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(100, reshuffle_each_iteration=True).repeat().batch(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    '''We use a simple model with only one hidden fully connected layer.\n",
    "    More complex architectures can achieve higher accuracies, see http://yann.lecun.com/exdb/mnist/.'''\n",
    "    def __init__(self, units=128):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "        self.W1 = tf.layers.Dense(units, activation=tf.nn.relu, name=\"Layer1\")\n",
    "        self.W2 = tf.layers.Dense(10, name=\"Layer2\")\n",
    "    \n",
    "    def call(self, _input):\n",
    "        '''One forward pass of the model with a batch of MNIST images as the input x (shape: [32, 28x28x]) and \n",
    "        logits for each class label as the output (shape: [32, 10]).'''\n",
    "        x = tf.layers.flatten(_input)\n",
    "        hidden1 = self.W1(x)\n",
    "        logits = self.W2(hidden1)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "def loss(logits, labels):\n",
    "    '''Cross entropy loss between the predicted (soft) assignments and the true target labels.'''\n",
    "    return tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels))\n",
    "\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    batch_size = int(logits.shape[0])\n",
    "    return tf.reduce_sum(\n",
    "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size\n",
    "\n",
    "\n",
    "def train(model, optimizer, dataset, dataset_test, log_interval=100, stop=500):\n",
    "    \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\n",
    "    start = time.time()\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "        \n",
    "        # tape records operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images)\n",
    "            loss_value = loss(logits, labels)\n",
    "        \n",
    "        # compute the gradients and use them to optimize the model variables\n",
    "        grads = tape.gradient(loss_value, model.variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        \n",
    "        if log_interval and batch % log_interval == 0:\n",
    "            rate = log_interval / (time.time() - start) if batch > 0 else 1./(time.time() - start)\n",
    "            print('Step #%d\\tLoss: %.6f (%d steps/sec)' % (batch, loss_value, rate))\n",
    "            start = time.time()\n",
    "        if batch % 100 == 0:\n",
    "            test(model, dataset_test)\n",
    "        if batch >= stop: break\n",
    "        \n",
    "def test(model, dataset):\n",
    "    \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    for (images, labels) in dataset:\n",
    "        logits = model(images)\n",
    "        avg_loss = loss(logits, labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, axis=1, output_type=tf.int32),labels), tf.float32))\n",
    "        break\n",
    "    print('Test set: Average loss: {0:.3f}, Accuracy: {1:.1f}%\\n'.format(avg_loss.numpy(), 100 * accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the optimizer\n",
    "In this exercise you will implement three of the most used optimizer in deep learning from scratch: SGD, Adagrad, and Adam. This blog post (http://ruder.io/optimizing-gradient-descent/) gives a great overview over the different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "$$ \n",
    "    \\theta^+ = \\theta - \\eta \\nabla L(x_{i:i+n}; \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def apply_gradients(self, grads_and_vars):        \n",
    "        for gradient, variable in grads_and_vars:\n",
    "            variable.assign_sub(self.learning_rate*gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad \n",
    "$$\n",
    "    g_{t,i} = \\nabla_\\theta L(x_t; \\theta_{t,i})\\\\\n",
    "    G_{t,i} = \\sum_{\\tau=0}^t g_{\\tau, i}^2\\\\\n",
    "    \\theta^+ = \\theta - \\frac{\\eta}{\\sqrt{G_{t,i} + \\epsilon}} g_{t,i}\n",
    "$$\n",
    "initial_accumulator_value $:= \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, learning_rate=0.001, initial_accumulator_value=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_accumulator_value = initial_accumulator_value\n",
    "        self.g = None\n",
    "        \n",
    "    def apply_gradients(self, grads_and_vars):\n",
    "        if self.g is None:\n",
    "            grads_and_vars = list(grads_and_vars)\n",
    "            self.g = [tf.ones_like(gradient_layer, dtype=tf.float64) * self.initial_accumulator_value for (gradient_layer, _) in grads_and_vars]\n",
    "            \n",
    "        for i, (gradient, variable) in enumerate(grads_and_vars):\n",
    "            self.g[i] += tf.square(gradient)\n",
    "            variable.assign_sub(self.learning_rate/tf.sqrt(self.g[i])*gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "https://arxiv.org/pdf/1412.6980.pdf see improved algorithm at the end of section 2.\n",
    "$$\n",
    "    m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta L(x_t; \\theta_t)\\\\\n",
    "    v_t = \\beta_2 v_{t-1} + (1-\\beta_2) \\left(\\nabla_\\theta L(x_t; \\theta_t)\\right) ^2\\\\\n",
    "    \\eta_t = \\eta * \\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t}\n",
    "$$\n",
    "$$\n",
    "    \\theta_{t+1} = \\theta_t - \\frac{\\eta_t}{\\sqrt{v_t} + \\epsilon}m_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def apply_gradients(self, grads_and_vars):\n",
    "        self.iteration += 1\n",
    "        if self.m is None:\n",
    "            grads_and_vars = list(grads_and_vars)\n",
    "            self.m = [tf.zeros_like(gradient_layer, dtype=tf.float64) for (gradient_layer, _) in grads_and_vars]\n",
    "            self.v = [tf.zeros_like(gradient_layer, dtype=tf.float64) for (gradient_layer, _) in grads_and_vars]\n",
    "\n",
    "        learning_rate = self.learning_rate * tf.cast(tf.sqrt(1-self.beta2**self.iteration)/(1-self.beta1**self.iteration), dtype=tf.float64)\n",
    "        \n",
    "        for i, (gradient, variable) in enumerate(grads_and_vars):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradient\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * tf.square(gradient)\n",
    "            update_step = learning_rate / (tf.sqrt(self.v[i]) + self.epsilon) * self.m[i]\n",
    "            variable.assign_sub(update_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #0\tLoss: 2.352876 (23 steps/sec)\n",
      "Test set: Average loss: 2.283, Accuracy: 35.2%\n",
      "\n",
      "Step #100\tLoss: 0.581958 (153 steps/sec)\n",
      "Test set: Average loss: 0.373, Accuracy: 88.7%\n",
      "\n",
      "Step #200\tLoss: 0.258020 (149 steps/sec)\n",
      "Test set: Average loss: 0.327, Accuracy: 90.0%\n",
      "\n",
      "Step #300\tLoss: 0.113682 (156 steps/sec)\n",
      "Test set: Average loss: 0.338, Accuracy: 87.9%\n",
      "\n",
      "Step #400\tLoss: 0.198193 (154 steps/sec)\n",
      "Test set: Average loss: 0.272, Accuracy: 91.2%\n",
      "\n",
      "Step #500\tLoss: 0.157352 (145 steps/sec)\n",
      "Test set: Average loss: 0.278, Accuracy: 92.9%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed, so you can compare your implementation against the provided tensorflow optimizer.\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\"\"\"Use your own implementation of the optimizer.\"\"\"\n",
    "# optimizer = SGD(learning_rate=0.01)\n",
    "# optimizer = Adagrad(learning_rate=0.1)\n",
    "# optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "\"\"\"Use a pre-built one from tensorflow. Compare your solution against these given optimizers.\"\"\"\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "# optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "\n",
    "train(model, optimizer, dset_train, dset_test, stop=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
